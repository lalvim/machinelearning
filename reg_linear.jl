### A Pluto.jl notebook ###
# v0.12.6

using Markdown
using InteractiveUtils

# â•”â•â•¡ 8dbcf370-1dec-11eb-0a2d-e967ec1f2fde
md" ### Nomenclatura"

# â•”â•â•¡ c4a8db60-1dec-11eb-0c39-73a8aac79092
md"Seja $X$ uma matriz com dimensÃµes $n x m$ com $n$ linhas e $m$ colunas que contÃ©m nossos exemplos. $x_i^{(j)}$ corresponde ao elemento de $X$ em que $i$ Ã© uma linha da matriz (exemplo) e $j$ uma coluna da matriz (atributo), cujo acesso na linguagem representamos por x[i,j]. Para pegarmos um vetor linha desta matriz (exemplo) representamos formalmente por $X_i$ e na linguagem por x[i,:]."

# â•”â•â•¡ cd532e00-1dec-11eb-06d9-9b7c76e9c1ba
md"Seja $\theta$ um vetor linha com $m$ elementos tal que $m$ corresponde ao nÃºmero de atributos mais um. $\theta = [\theta^{(0)},\theta^{(1)}, ...,\theta^{(m)}]$ em que cada elemento corresponde ao j-Ã©simo parÃ¢metro associado ao j-Ã©simo atributo. Na nossa linguagem acessamos um elemento por $\theta[i]$ e o vetor por $\theta$"

# â•”â•â•¡ dc365e10-1dec-11eb-0e4f-95624ee468ea
md"Seja $y$ nosso vetor alvo em que representamos o i-Ã©simo elemento por $y_i$. Na nossa linguagem representamos por $y[i]$."

# â•”â•â•¡ 571a5ea0-1dee-11eb-0eaf-bf746a6faa86
md"### Algumas libs importantes usadas neste experimento"

# â•”â•â•¡ 317bbe60-1ded-11eb-2faa-c3e47c0df176
begin 
   import Statistics: mean,std
   import BenchmarkTools: @benchmark, @btime
   import StaticArrays: @SMatrix
   import Plots: plot,plot!,scatter,scatter!	
   import StatsBase: zscore	
   Î¼ = mean	
end

# â•”â•â•¡ f9af3a70-1dec-11eb-34f0-91b7fc591b87
md" ### HipÃ³tese e modelo linear"

# â•”â•â•¡ 78d2c0e0-1f98-11eb-06bc-b51c3b4b9f35
md"A seguir descrevemos a hipÃ³tese que consiste da aplicaÃ§Ã£o de um modelo (conjunto de parÃ¢metros) em um exemplo. Note que a hipÃ³tese sempre se refere Ã  aplicaÃ§Ã£o de um Ãºnico exemplo e nÃ£o ao conjunto de dados todo. "

# â•”â•â•¡ 2e7709e0-1ded-11eb-2abd-d770bf1afe8a
md"$H_{\theta}(X_i) = \sum_{j}^{m}{x_{i}^{(j)} \theta^{(j)}} = X_i\theta^{T}$"

# â•”â•â•¡ b05f9930-1f97-11eb-0083-07ebd21313e0
md"Exemplificando o produto de vetores, nossa hipÃ³tese faria o seguinte cÃ¡lculo para um exemplo $X_i = \begin{bmatrix} 1 & 500 \end{bmatrix}$ e um vetor de parÃ¢metros $\theta^T = \begin{bmatrix}
    0.5 \\
    1.0
\end{bmatrix}$:"

# â•”â•â•¡ cbf998e0-1f96-11eb-0a8b-a3bf631bfe02
md"e.g. $H_{\theta}(X_i) = \begin{bmatrix}
    1 & 500 
  \end{bmatrix}\begin{bmatrix}
    0.5 \\
    1.0
\end{bmatrix} = 500.5$"

# â•”â•â•¡ 4cc76c80-1f98-11eb-0c70-29aec0b92ac4
md"Note que o resultado Ã© um escalar, pois estamos estimando um valor contÃ­nuo (problema de regressÃ£o)."

# â•”â•â•¡ c0b094a0-1f98-11eb-2efb-07f680447b12
md"### ImplementaÃ§Ã£o a hipÃ³tese"

# â•”â•â•¡ be0deb50-1ded-11eb-06bb-33e94217e815
H(Ï‡,Î¸) = Ï‡*Î˜'

# â•”â•â•¡ 2f5a6500-1ded-11eb-11dd-bd88bf102b98
md" ### FunÃ§Ã£o de custo"

# â•”â•â•¡ 30972ac0-1ded-11eb-1007-1d3833dfc264

md"$J(\theta)= \frac{1}{2n}\sum_{i=1}^{n}{(H_{\theta}(X_i) - y_i)Â²}$"


# â•”â•â•¡ bba6d590-1e1d-11eb-2df6-67bd6c5d4777
md" A seguir estou calculando a funÃ§Ã£o de custo sem loop, apenas por matrizes.Desta maneira, geramos em uma multiplicaÃ§Ã£o de matrizes, um vetor de prediÃ§Ãµes com a mesma dimensÃ£o da saÃ­da. ApÃ³s isto, subtraÃ­mos elemento a elemento. ApÃ³s isto, elevamos cada elemento ao quadrado e depois tiramos uma mÃ©dia. Explicado na aula anterior."

# â•”â•â•¡ 9837a3d0-1ded-11eb-3c49-17a45387e46e
J(Î˜,Ï‡,Î³) = .5Î¼((Ï‡*Î˜' .- Î³).^2)

# â•”â•â•¡ 2ebaad22-1dee-11eb-2fb1-f51c49ec13fb
md"### Gradiente descendente"

# â•”â•â•¡ d1cf31b0-1e17-11eb-22d1-916c9a331700
md"A seguir, mostramos o cÃ¡lculo do gradiente. Como podemos notar, o expoente 2 desce e Ã© eliminado pela constante 2 do denominador. Adicionalemente, vemos que Ã© uma subtraÃ§Ã£o do gradiente, pois estamos minimizando a funÃ§Ã£o de custo. Por isto, estamos caminhando no sentido contrÃ¡rio ao do gradiente. Ã‰ importante tambÃ©m ressaltar que o viÃ©s jÃ¡ estÃ¡ embutido na matriz $X$ na forma de uma coluna com valores 1. Desta forma, o cÃ¡lculo do gradiente Ã© o mesmo para o parÃ¢metro do viÃ©s ($Î¸^{(0)}$) e os demais." 

# â•”â•â•¡ cbf654c0-1dfa-11eb-0994-85a67ded79b0
md"$Î¸^{(j)} = Î¸^{(j)} - Î± \frac{1}{n} \sum_{i=1}^{n}{(H_Î¸(X_i) - y_i)x_{i}^{(j)}}$"

# â•”â•â•¡ 46484b80-1e1d-11eb-2bdc-e50b24f6b530
md"Entretanto, podemos condensar esta fÃ³rmula usando notaÃ§Ã£o de vetores e matrizes. Lembrando que $\theta$ Ã© um vetor agora e os cÃ¡lculos sÃ£o feitos simultaneamente para cada eleemnto deste. Podemos observar que o Ã­ndice da coluna some, pois sÃ£o processadas automaticamente. A seguir, veja a fÃ³rmula:"

# â•”â•â•¡ f1089b10-1e1d-11eb-120e-cdbd993d72dc
md"$Î¸ = Î¸ - Î± \frac{1}{n} \sum_{i=1}^{n}{(H_Î¸(X_i) - y_i) \circ X_{i}}$"

# â•”â•â•¡ 79bc38d0-1e1f-11eb-066c-2929340c479e
md"Cabe observar que $\circ$ Ã© o produto de hadamard."

# â•”â•â•¡ f5d33090-1e1f-11eb-3f25-d10f2616803d
md"### Algoritmo "

# â•”â•â•¡ 7a22eab0-1def-11eb-2672-495fb9cf19f8
function train(Î¸,Ï‡,Î³;verbose=false,Î± = 1e-3,Ïµ=1e-5,Ï„=1e3)
	
	eâ‚‚  = J(Î¸,Ï‡,Î³) 
	eâ‚  = eâ‚‚ + .1
	#Î”â‚,Î”â‚‚ = eâ‚ - eâ‚‚ + 2Ïµ,eâ‚ - eâ‚‚
	ğŸ’» = [[eâ‚‚ Î¸]]
	i   = 1
	while eâ‚‚ < eâ‚ && eâ‚‚ > Ïµ && i < Ï„    
	#while Î”â‚ > Î”â‚‚  && eâ‚‚ > Ïµ && i < Ï„     
		
		âˆ‡     = Î¼((Ï‡*Î¸' .- Î³).* Ï‡,dims=1)
		Î¸    -= Î± .* âˆ‡
		
		eâ‚,eâ‚‚ = eâ‚‚,J(Î¸,Ï‡,Î³)
		#Î”â‚,Î”â‚‚ = Î”â‚‚,eâ‚ - eâ‚‚
		verbose && if (i%10==0) append!(ğŸ’»,[[eâ‚‚ Î¸]]) end
		
		i += 1
	end
	Î¸,vcat(ğŸ’»...)
end	

# â•”â•â•¡ 1f0e51f0-1df4-11eb-0feb-b53552fa63c9
md"### Testes de unitÃ¡rios"

# â•”â•â•¡ 58dd7720-1e09-11eb-1904-af5f9991ece5
md" Sempre que for desenvolver suas funÃ§Ãµes, realize uma bateria de testes para cada uma antes mesmo de executar todo o seu programa. Isso vai lhe poupar muito tempo para uma futura depuraÃ§Ã£o de cÃ³digo. Assim, para cada funÃ§Ã£o, faÃ§a pelo menos um teste. Elabore casos bons de entrada para suas funÃ§Ãµes de forma que cubra bem o espaÃ§o de possibilidades. Seus casos devem ser bem simples para que num futuro, se der erro, vocÃª consiga rapidamente resolver."

# â•”â•â•¡ 259ac040-1e39-11eb-3670-f5c5519a1978
md"### Padronizando os Dados "

# â•”â•â•¡ 246cc792-1e39-11eb-3c20-f1623d8ea737
md"O objetivo de padronizar os dados Ã© acelerar a convergÃªncia do mÃ©todo. Aqui utilizaremos a padronizaÃ§Ã£o zscore. Entretanto, para amostras pequenas, a padronizaÃ§Ã£o nÃ£o funciona muito bem."

# â•”â•â•¡ c467ca00-1e08-11eb-3d41-75a38532652b
md"### Verificando a curva de erro"

# â•”â•â•¡ d1a88a00-1f8f-11eb-36b7-f39dabee0657
md"### Veificando o comportamento dos parÃ¢metros"

# â•”â•â•¡ b405694e-1f8f-11eb-0fad-5d0c3527d206
md"### Verificando o modelo"

# â•”â•â•¡ d497051e-1f94-11eb-2ccb-a7d3e52570d1
md"Como podemos observar, o gradiente jogou o Î¸â‚ para zero (viÃ©s, responsÃ¡vel pela translaÃ§Ã£o) e ajustou o Î¸â‚‚ (responsÃ¡vel pela rotaÃ§Ã£o). Cabe ressaltar que estamos trabalhando com os dados padronizados. Ã‰ possÃ­vel, no entanto, despadronizar e obter prediÃ§Ãµes no campo dos nÃºmeros originais."

# â•”â•â•¡ 37c52e40-1e48-11eb-1e13-7ba1bebfabc4
md"### DepuraÃ§Ã£o "

# â•”â•â•¡ 44997b82-1e48-11eb-1821-f7f3aecd8093
md" Costumo colocar alguma variÃ¡veis soltas para que eu veja se estÃ¡ tudo correto. "

# â•”â•â•¡ a419b220-1e4b-11eb-1299-295bf39df3f4
md"### SoluÃ§Ã£o direta por Ã¡lgebra linear"

# â•”â•â•¡ a39cb9f0-1e4b-11eb-296e-bf19e5438416
train(X,Y) = inv(X'X)X'Y

# â•”â•â•¡ 628624c0-1dfa-11eb-2afb-137bf77669e1
begin
	Î˜1 = [0. 4.] 
	Ï‡ =  [1 50.;1 60.;1 100.; 1 200.]
	Î³ =  [200  ;   240  ;   400; 800]
	@assert J(Î˜1,Ï‡,Î³) == 0.0
	@assert train(Î˜1,Ï‡,Î³)[1] == [0. 4.] # iniciando com a soluÃ§Ã£o, o Î¸ deve continuar igual
end	

# â•”â•â•¡ 4c632fee-1e39-11eb-00c6-c3f2a0774005
begin 
	Ï‡â‚š = hcat(ones((4,1)), zscore(Ï‡[:,2:end])) # padronizamos sÃ³ o que nÃ£o Ã© viÃ©s
	Î³â‚š = zscore(Î³)
end	

# â•”â•â•¡ d8ab5330-1e47-11eb-06b3-af3d2ee97c7b
Ï‡â‚š

# â•”â•â•¡ 86be2920-1e48-11eb-10f1-6740a2459951
Î³â‚š

# â•”â•â•¡ 74604cb2-1e05-11eb-19ef-09fd04dc52e3
(Î¸áµ,r) = train([4 5],Ï‡â‚š,Î³â‚š,verbose=true,Î± = 1e-3,Ïµ=1e-5,Ï„=1e5);

# â•”â•â•¡ b7913600-1e0c-11eb-362e-0b4b9ef9d1aa
plot(1:length(r[:,1]),r[:,1],xlabel="iteraÃ§Ãµes",ylabel="J(Î¸)",title="AnÃ¡lise de erros",label="Erro")

# â•”â•â•¡ fa0bd460-1e23-11eb-131f-f31bec373c07
begin
	p = plot(1:length(r[:,2]),r[:,2],xlabel="iteraÃ§Ãµes",ylabel="Î¸â‚,Î¸â‚‚",title="AnÃ¡lise de parÃ¢metros",label="Î¸â‚")
	plot!(p,1:length(r[:,3]),r[:,3],xlabel="iteraÃ§Ãµes",label="Î¸â‚‚")
end	

# â•”â•â•¡ cc73d620-1f8f-11eb-357d-b19b50c19280
begin
	f(x) = Î¸áµ[1] + Î¸áµ[2]*x # o mesmo que H(X) lÃ¡ do inÃ­cio
	plot(Ï‡â‚š[:,2],f,xlabel="mÂ² (padronizado)",ylabel="PreÃ§o (padronizado)",title="AnÃ¡lise do da prediÃ§Ã£o do modelo",label="Î¸â‚ + Î¸â‚‚x")
	scatter!(Ï‡â‚š[:,2],Î³â‚š,label="Dados Coletados")
end	

# â•”â•â•¡ 41505b10-1e34-11eb-06db-750eb3ab11cf
r[end,:]

# â•”â•â•¡ 29b04780-1e44-11eb-3bcf-8b5f121d157f
J(Î¸áµ,Ï‡â‚š,Î³â‚š)

# â•”â•â•¡ d0de29e0-1e36-11eb-398c-5f0b55ea281e
Î¸áµ

# â•”â•â•¡ 70775a70-1e4c-11eb-3cec-11fd54e9b637
train(Ï‡â‚š,Î³â‚š) 

# â•”â•â•¡ Cell order:
# â•Ÿâ”€8dbcf370-1dec-11eb-0a2d-e967ec1f2fde
# â•Ÿâ”€c4a8db60-1dec-11eb-0c39-73a8aac79092
# â•Ÿâ”€cd532e00-1dec-11eb-06d9-9b7c76e9c1ba
# â•Ÿâ”€dc365e10-1dec-11eb-0e4f-95624ee468ea
# â•Ÿâ”€571a5ea0-1dee-11eb-0eaf-bf746a6faa86
# â• â•317bbe60-1ded-11eb-2faa-c3e47c0df176
# â•Ÿâ”€f9af3a70-1dec-11eb-34f0-91b7fc591b87
# â•Ÿâ”€78d2c0e0-1f98-11eb-06bc-b51c3b4b9f35
# â•Ÿâ”€2e7709e0-1ded-11eb-2abd-d770bf1afe8a
# â•Ÿâ”€b05f9930-1f97-11eb-0083-07ebd21313e0
# â•Ÿâ”€cbf998e0-1f96-11eb-0a8b-a3bf631bfe02
# â•Ÿâ”€4cc76c80-1f98-11eb-0c70-29aec0b92ac4
# â•Ÿâ”€c0b094a0-1f98-11eb-2efb-07f680447b12
# â• â•be0deb50-1ded-11eb-06bb-33e94217e815
# â•Ÿâ”€2f5a6500-1ded-11eb-11dd-bd88bf102b98
# â•Ÿâ”€30972ac0-1ded-11eb-1007-1d3833dfc264
# â•Ÿâ”€bba6d590-1e1d-11eb-2df6-67bd6c5d4777
# â• â•9837a3d0-1ded-11eb-3c49-17a45387e46e
# â•Ÿâ”€2ebaad22-1dee-11eb-2fb1-f51c49ec13fb
# â•Ÿâ”€d1cf31b0-1e17-11eb-22d1-916c9a331700
# â•Ÿâ”€cbf654c0-1dfa-11eb-0994-85a67ded79b0
# â•Ÿâ”€46484b80-1e1d-11eb-2bdc-e50b24f6b530
# â•Ÿâ”€f1089b10-1e1d-11eb-120e-cdbd993d72dc
# â•Ÿâ”€79bc38d0-1e1f-11eb-066c-2929340c479e
# â•Ÿâ”€f5d33090-1e1f-11eb-3f25-d10f2616803d
# â• â•7a22eab0-1def-11eb-2672-495fb9cf19f8
# â•Ÿâ”€1f0e51f0-1df4-11eb-0feb-b53552fa63c9
# â•Ÿâ”€58dd7720-1e09-11eb-1904-af5f9991ece5
# â• â•628624c0-1dfa-11eb-2afb-137bf77669e1
# â•Ÿâ”€259ac040-1e39-11eb-3670-f5c5519a1978
# â•Ÿâ”€246cc792-1e39-11eb-3c20-f1623d8ea737
# â• â•4c632fee-1e39-11eb-00c6-c3f2a0774005
# â•Ÿâ”€c467ca00-1e08-11eb-3d41-75a38532652b
# â• â•74604cb2-1e05-11eb-19ef-09fd04dc52e3
# â•Ÿâ”€b7913600-1e0c-11eb-362e-0b4b9ef9d1aa
# â•Ÿâ”€d1a88a00-1f8f-11eb-36b7-f39dabee0657
# â•Ÿâ”€fa0bd460-1e23-11eb-131f-f31bec373c07
# â•Ÿâ”€b405694e-1f8f-11eb-0fad-5d0c3527d206
# â•Ÿâ”€d497051e-1f94-11eb-2ccb-a7d3e52570d1
# â• â•cc73d620-1f8f-11eb-357d-b19b50c19280
# â•Ÿâ”€37c52e40-1e48-11eb-1e13-7ba1bebfabc4
# â•Ÿâ”€44997b82-1e48-11eb-1821-f7f3aecd8093
# â• â•41505b10-1e34-11eb-06db-750eb3ab11cf
# â• â•29b04780-1e44-11eb-3bcf-8b5f121d157f
# â• â•d0de29e0-1e36-11eb-398c-5f0b55ea281e
# â• â•d8ab5330-1e47-11eb-06b3-af3d2ee97c7b
# â• â•86be2920-1e48-11eb-10f1-6740a2459951
# â•Ÿâ”€a419b220-1e4b-11eb-1299-295bf39df3f4
# â• â•a39cb9f0-1e4b-11eb-296e-bf19e5438416
# â• â•70775a70-1e4c-11eb-3cec-11fd54e9b637
