### A Pluto.jl notebook ###
# v0.12.11

using Markdown
using InteractiveUtils

# â•”â•â•¡ 8dbcf370-1dec-11eb-0a2d-e967ec1f2fde
md" ### Nomenclatura"

# â•”â•â•¡ c4a8db60-1dec-11eb-0c39-73a8aac79092
md"Seja $X$ uma matriz com dimensÃµes $n x m$ com $n$ linhas e $m$ colunas que contÃ©m nossos exemplos. $x_i^{(j)}$ corresponde ao elemento de $X$ em que $i$ Ã© uma linha da matriz (exemplo) e $j$ uma coluna da matriz (atributo), cujo acesso na linguagem representamos por x[i,j]. Para pegarmos um vetor linha desta matriz (exemplo) representamos formalmente por $X_i$ e na linguagem por x[i,:]."

# â•”â•â•¡ cd532e00-1dec-11eb-06d9-9b7c76e9c1ba
md"Seja $\theta$ um vetor linha com $m$ elementos tal que $m$ corresponde ao nÃºmero de atributos mais um. $\theta = [\theta^{(0)},\theta^{(1)}, ...,\theta^{(m)}]$ em que cada elemento corresponde ao j-Ã©simo parÃ¢metro associado ao j-Ã©simo atributo. Na nossa linguagem acessamos um elemento por $\theta[i]$ e o vetor por $\theta$"

# â•”â•â•¡ dc365e10-1dec-11eb-0e4f-95624ee468ea
md"Seja $y$ nosso vetor alvo em que representamos o i-Ã©simo elemento por $y_i$. Na nossa linguagem representamos por $y[i]$."

# â•”â•â•¡ 571a5ea0-1dee-11eb-0eaf-bf746a6faa86
md"### Algumas libs importantes usadas neste experimento"

# â•”â•â•¡ 317bbe60-1ded-11eb-2faa-c3e47c0df176
begin 
   import Statistics: mean,std
   import BenchmarkTools: @benchmark, @btime
   import StaticArrays: @SMatrix
   import Plots: plot,plot!,scatter,scatter!	
   import StatsBase: zscore
   Î¼ = mean	
end

# â•”â•â•¡ f9af3a70-1dec-11eb-34f0-91b7fc591b87
md" ### HipÃ³tese e modelo de regressÃ£o logÃ­stica"

# â•”â•â•¡ 78d2c0e0-1f98-11eb-06bc-b51c3b4b9f35
md"A seguir descrevemos a hipÃ³tese que consiste da aplicaÃ§Ã£o de um modelo (conjunto de parÃ¢metros) em um exemplo. Note que a hipÃ³tese sempre se refere Ã  aplicaÃ§Ã£o de um Ãºnico exemplo e nÃ£o ao conjunto de dados todo. Neste caso, aplicamos um modelo linear $z$ como expoente da funÃ§Ã£o logÃ­stica, cujo objetivo Ã© delimitar a saÃ­da num intervalo entre 0 e 1."

# â•”â•â•¡ 2e7709e0-1ded-11eb-2abd-d770bf1afe8a
md"$H_{\theta}(X_i) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-X_i \theta^T }} = P(Y_i|X_i;\theta)$"

# â•”â•â•¡ 9da10890-2aac-11eb-0069-15538564fb84
begin
	x = -10:0.1:10
	plot(x,1 ./(1 .+ â„¯.^(-x)),label="logit")
end	

# â•”â•â•¡ b05f9930-1f97-11eb-0083-07ebd21313e0
md"Exemplificando o produto de vetores, nossa hipÃ³tese faria o seguinte cÃ¡lculo para um exemplo $X_i = \begin{bmatrix} 1 & 2 \end{bmatrix}$ e um vetor de parÃ¢metros $\theta^T = \begin{bmatrix}
    0.5 \\
    1.0
\end{bmatrix}$:"

# â•”â•â•¡ cbf998e0-1f96-11eb-0a8b-a3bf631bfe02
md"e.g. $z = \begin{bmatrix}
    1 & 2 
  \end{bmatrix}\begin{bmatrix}
    0.5 \\
    1.0
\end{bmatrix} = 2.5$ entÃ£o teremos $\frac{1}{1 + e^{-z}}$ = $(1 ./(1 .+ â„¯.^(-2.5)))"

# â•”â•â•¡ 4cc76c80-1f98-11eb-0c70-29aec0b92ac4
md"Note que o resultado de $z$ Ã© maior que 1. Entretanto, a funÃ§Ã£o logÃ­stica coloca no intervalo entre 0 e 1."

# â•”â•â•¡ c0b094a0-1f98-11eb-2efb-07f680447b12
md"### ImplementaÃ§Ã£o a hipÃ³tese"

# â•”â•â•¡ be0deb50-1ded-11eb-06bb-33e94217e815
H(Ï‡,Î¸) = vec(1.0 ./(1.0 .+ â„¯ .^(-Ï‡*Î¸'))); #vec: garantir que seja um vetor coluna

# â•”â•â•¡ 2f5a6500-1ded-11eb-11dd-bd88bf102b98
md" ### FunÃ§Ã£o de custo"

# â•”â•â•¡ 30972ac0-1ded-11eb-1007-1d3833dfc264

md"$J(\theta)= -\frac{1}{n}[ \sum_{i=1}^{n}{y_i log(P(Y_i|X_i;\theta)) + (1 - y_i )log(1 - P(Y_i|X_i;\theta))]  }$"


# â•”â•â•¡ bba6d590-1e1d-11eb-2df6-67bd6c5d4777
md" A seguir estou calculando a funÃ§Ã£o de custo sem loop, apenas por matrizes.Desta maneira, geramos em uma multiplicaÃ§Ã£o de matrizes, um vetor de prediÃ§Ãµes com a mesma dimensÃ£o da saÃ­da. ApÃ³s isto, subtraÃ­mos elemento a elemento. ApÃ³s isto, elevamos cada elemento ao quadrado e depois tiramos uma mÃ©dia. Explicado na aula anterior."

# â•”â•â•¡ 9837a3d0-1ded-11eb-3c49-17a45387e46e
function J(py::Union{Array{Float64,1},Float64},Î³::Union{Array{Float64,1},Float64}) 
	Î¼(-Î³ .* log.(2,py .+ 1e-4) .- (1 .- Î³) .* (log.(2,1e-4+1 .- py)))
end	

# â•”â•â•¡ 86e80760-2ac1-11eb-0326-7f2448320bc2
begin
	py  = collect(0.01:0.01:1)
	y0  = zeros(length(py))
	y1  = ones(length(py))
    plot(py,J.(py,y0),label="y=0",ylabel="J",xlabel="P(yáµ¢)")
	plot!(py,J.(py,y1),label="y=1")
end	

# â•”â•â•¡ 2ebaad22-1dee-11eb-2fb1-f51c49ec13fb
md"### Gradiente descendente"

# â•”â•â•¡ d1cf31b0-1e17-11eb-22d1-916c9a331700
md"A seguir, mostramos o cÃ¡lculo do gradiente. Como podemos notar, o expoente 2 desce e Ã© eliminado pela constante 2 do denominador. Adicionalemente, vemos que Ã© uma subtraÃ§Ã£o do gradiente, pois estamos minimizando a funÃ§Ã£o de custo. Por isto, estamos caminhando no sentido contrÃ¡rio ao do gradiente. Ã‰ importante tambÃ©m ressaltar que o viÃ©s jÃ¡ estÃ¡ embutido na matriz $X$ na forma de uma coluna com valores 1. Desta forma, o cÃ¡lculo do gradiente Ã© o mesmo para o parÃ¢metro do viÃ©s ($Î¸^{(0)}$) e os demais." 

# â•”â•â•¡ cbf654c0-1dfa-11eb-0994-85a67ded79b0
md"$Î¸^{(j)} = Î¸^{(j)} - Î± \frac{1}{n} \sum_{i=1}^{n}{(H_Î¸(X_i) - y_i)x_{i}^{(j)}}$"

# â•”â•â•¡ 46484b80-1e1d-11eb-2bdc-e50b24f6b530
md"Entretanto, podemos condensar esta fÃ³rmula usando notaÃ§Ã£o de vetores e matrizes. Lembrando que $\theta$ Ã© um vetor agora e os cÃ¡lculos sÃ£o feitos simultaneamente para cada eleemnto deste. Podemos observar que o Ã­ndice da coluna some, pois sÃ£o processadas automaticamente. A seguir, veja a fÃ³rmula:"

# â•”â•â•¡ f1089b10-1e1d-11eb-120e-cdbd993d72dc
md"$Î¸ = Î¸ - Î± \frac{1}{n} \sum_{i=1}^{n}{(H_Î¸(X_i) - y_i) \circ X_{i}}$"

# â•”â•â•¡ 79bc38d0-1e1f-11eb-066c-2929340c479e
md"Cabe observar que $\circ$ Ã© o produto de hadamard."

# â•”â•â•¡ f5d33090-1e1f-11eb-3f25-d10f2616803d
md"### Algoritmo "

# â•”â•â•¡ 7a22eab0-1def-11eb-2672-495fb9cf19f8
function train(Î¸,Ï‡,Î³;verbose=false,Î± = 1e-3,Ïµ=1e-5,Ï„=1e3)
	eâ‚‚  = J(H(Ï‡,Î¸),Î³) 
	eâ‚  = eâ‚‚ + .1
	#Î”â‚,Î”â‚‚ = eâ‚ - eâ‚‚ + 2Ïµ,eâ‚ - eâ‚‚
	ğŸ’» = [[eâ‚‚ Î¸]]
	i   = 1
	#while eâ‚‚ > Ïµ && i < Ï„
	while eâ‚ > eâ‚‚ && eâ‚‚ > Ïµ && i < Ï„    
    #while Î”â‚ > Î”â‚‚  && eâ‚‚ > Ïµ && i < Ï„     
		âˆ‡     = Î¼((H(Ï‡,Î¸) .- Î³).* Ï‡,dims=1)
		Î¸    -= Î± .* âˆ‡
		eâ‚,eâ‚‚ = eâ‚‚,J(H(Ï‡,Î¸),Î³)
		#Î”â‚,Î”â‚‚ = Î”â‚‚,eâ‚ - eâ‚‚
		verbose && if (i%10==0) append!(ğŸ’»,[[eâ‚‚ Î¸]]) end
		
		i += 1
	end
	
	Î¸,vcat(ğŸ’»...)
end	

# â•”â•â•¡ 1f0e51f0-1df4-11eb-0feb-b53552fa63c9
md"### Testes de unitÃ¡rios (funÃ§Ã£o OU)"

# â•”â•â•¡ 58dd7720-1e09-11eb-1904-af5f9991ece5
md" Sempre que for desenvolver suas funÃ§Ãµes, realize uma bateria de testes para cada uma antes mesmo de executar todo o seu programa. Isso vai lhe poupar muito tempo para uma futura depuraÃ§Ã£o de cÃ³digo. Assim, para cada funÃ§Ã£o, faÃ§a pelo menos um teste. Elabore casos bons de entrada para suas funÃ§Ãµes de forma que cubra bem o espaÃ§o de possibilidades. Seus casos devem ser bem simples para que num futuro, se der erro, vocÃª consiga rapidamente resolver."

# â•”â•â•¡ 628624c0-1dfa-11eb-2afb-137bf77669e1
begin
	Ï‡ = [1.0 1.0 1.0; 
		1 0 1; 
		1 1 0; 
		1 0 0;
		1 0.3 0.4;
		1 0.5 0.6;
		1 0.8 0.4;
		1 0.4 0.1
	]
	Î³ = [1. ;1.; 1.; 0.; 0 ; 1 ; 1; 0]
	Î˜ = [0.0 0 0]
	@assert J(H(Ï‡,Î˜),Î³) == 0.9997114898418766
	@assert H(Ï‡,Î˜) == [0.5 ; 0.5 ; 0.5;  0.5 ; 0.5 ; 0.5;  0.5;  0.5]
	@assert (H(Ï‡,Î˜) .- Î³) == [-0.5;  -0.5;  -0.5;  0.5;  0.5;  -0.5;  -0.5;  0.5]
	
end	

# â•”â•â•¡ 259ac040-1e39-11eb-3670-f5c5519a1978
md"### Padronizando os Dados "

# â•”â•â•¡ 246cc792-1e39-11eb-3c20-f1623d8ea737
md"O objetivo de padronizar os dados Ã© acelerar a convergÃªncia do mÃ©todo. Aqui utilizaremos a padronizaÃ§Ã£o zscore. Entretanto, para amostras pequenas, a padronizaÃ§Ã£o nÃ£o funciona muito bem."

# â•”â•â•¡ 4c632fee-1e39-11eb-00c6-c3f2a0774005
begin 
	Ï‡â‚š = hcat(ones((8,1)), zscore(Ï‡[:,2:end])) # padronizamos sÃ³ o que nÃ£o Ã© viÃ©s
end	

# â•”â•â•¡ c467ca00-1e08-11eb-3d41-75a38532652b
md"### Verificando a curva de erro"

# â•”â•â•¡ 74604cb2-1e05-11eb-19ef-09fd04dc52e3
(Î¸áµ,r) = train([.1 .0 0],Ï‡,Î³,verbose=true;Î± = 0.5,Ïµ=1e-2,Ï„=1e4);

# â•”â•â•¡ b7913600-1e0c-11eb-362e-0b4b9ef9d1aa
plot(1:length(r[:,1]),r[:,1],xlabel="iteraÃ§Ãµes",ylabel="J(Î¸)",title="AnÃ¡lise de erros",label="Erro")

# â•”â•â•¡ d1a88a00-1f8f-11eb-36b7-f39dabee0657
md"### Veificando o comportamento dos parÃ¢metros"

# â•”â•â•¡ fa0bd460-1e23-11eb-131f-f31bec373c07
begin
	plot(1:length(r[:,2]),r[:,2],xlabel="iteraÃ§Ãµes",ylabel="Î¸â‚,Î¸â‚‚,Î¸â‚ƒ",title="AnÃ¡lise de parÃ¢metros",label="Î¸â‚")
	plot!(1:length(r[:,3]),r[:,3],xlabel="iteraÃ§Ãµes",label="Î¸â‚‚")
	plot!(1:length(r[:,4]),r[:,4],xlabel="iteraÃ§Ãµes",label="Î¸â‚ƒ")
end	

# â•”â•â•¡ b405694e-1f8f-11eb-0fad-5d0c3527d206
md"### Testando as ClassificaÃ§Ãµes"

# â•”â•â•¡ d497051e-1f94-11eb-2ccb-a7d3e52570d1
md"Aqui, definiremos uma regra para classificaÃ§Ã£o."

# â•”â•â•¡ cc73d620-1f8f-11eb-357d-b19b50c19280
function classify(X::Array{Float64,2},Î¸::Array{Float64,2}) 
	H(X,Î¸)... > .5 ? 1 : 0
end	

# â•”â•â•¡ 803ccbc0-2b53-11eb-1399-cf96cbb67727
classify([1 0.5 0.4],Î¸áµ)

# â•”â•â•¡ 37c52e40-1e48-11eb-1e13-7ba1bebfabc4
md"### DepuraÃ§Ã£o "

# â•”â•â•¡ 44997b82-1e48-11eb-1821-f7f3aecd8093
md" Costumo colocar alguma variÃ¡veis soltas para que eu veja se estÃ¡ tudo correto. "

# â•”â•â•¡ 41505b10-1e34-11eb-06db-750eb3ab11cf
r[end,:]

# â•”â•â•¡ 29b04780-1e44-11eb-3bcf-8b5f121d157f
J(H(Î¸áµ,Ï‡),	Î³) 

# â•”â•â•¡ f81b6012-2b4a-11eb-173a-1bf763fbd71d
H(Î¸áµ,Ï‡)

# â•”â•â•¡ d0de29e0-1e36-11eb-398c-5f0b55ea281e
Î¸áµ

# â•”â•â•¡ d8ab5330-1e47-11eb-06b3-af3d2ee97c7b
Ï‡â‚š

# â•”â•â•¡ Cell order:
# â•Ÿâ”€8dbcf370-1dec-11eb-0a2d-e967ec1f2fde
# â•Ÿâ”€c4a8db60-1dec-11eb-0c39-73a8aac79092
# â•Ÿâ”€cd532e00-1dec-11eb-06d9-9b7c76e9c1ba
# â•Ÿâ”€dc365e10-1dec-11eb-0e4f-95624ee468ea
# â•Ÿâ”€571a5ea0-1dee-11eb-0eaf-bf746a6faa86
# â• â•317bbe60-1ded-11eb-2faa-c3e47c0df176
# â•Ÿâ”€f9af3a70-1dec-11eb-34f0-91b7fc591b87
# â•Ÿâ”€78d2c0e0-1f98-11eb-06bc-b51c3b4b9f35
# â•Ÿâ”€2e7709e0-1ded-11eb-2abd-d770bf1afe8a
# â• â•9da10890-2aac-11eb-0069-15538564fb84
# â•Ÿâ”€b05f9930-1f97-11eb-0083-07ebd21313e0
# â•Ÿâ”€cbf998e0-1f96-11eb-0a8b-a3bf631bfe02
# â•Ÿâ”€4cc76c80-1f98-11eb-0c70-29aec0b92ac4
# â•Ÿâ”€c0b094a0-1f98-11eb-2efb-07f680447b12
# â• â•be0deb50-1ded-11eb-06bb-33e94217e815
# â•Ÿâ”€2f5a6500-1ded-11eb-11dd-bd88bf102b98
# â•Ÿâ”€30972ac0-1ded-11eb-1007-1d3833dfc264
# â•Ÿâ”€bba6d590-1e1d-11eb-2df6-67bd6c5d4777
# â• â•9837a3d0-1ded-11eb-3c49-17a45387e46e
# â• â•86e80760-2ac1-11eb-0326-7f2448320bc2
# â•Ÿâ”€2ebaad22-1dee-11eb-2fb1-f51c49ec13fb
# â•Ÿâ”€d1cf31b0-1e17-11eb-22d1-916c9a331700
# â•Ÿâ”€cbf654c0-1dfa-11eb-0994-85a67ded79b0
# â•Ÿâ”€46484b80-1e1d-11eb-2bdc-e50b24f6b530
# â•Ÿâ”€f1089b10-1e1d-11eb-120e-cdbd993d72dc
# â•Ÿâ”€79bc38d0-1e1f-11eb-066c-2929340c479e
# â•Ÿâ”€f5d33090-1e1f-11eb-3f25-d10f2616803d
# â• â•7a22eab0-1def-11eb-2672-495fb9cf19f8
# â•Ÿâ”€1f0e51f0-1df4-11eb-0feb-b53552fa63c9
# â•Ÿâ”€58dd7720-1e09-11eb-1904-af5f9991ece5
# â• â•628624c0-1dfa-11eb-2afb-137bf77669e1
# â•Ÿâ”€259ac040-1e39-11eb-3670-f5c5519a1978
# â•Ÿâ”€246cc792-1e39-11eb-3c20-f1623d8ea737
# â• â•4c632fee-1e39-11eb-00c6-c3f2a0774005
# â• â•c467ca00-1e08-11eb-3d41-75a38532652b
# â• â•74604cb2-1e05-11eb-19ef-09fd04dc52e3
# â• â•b7913600-1e0c-11eb-362e-0b4b9ef9d1aa
# â•Ÿâ”€d1a88a00-1f8f-11eb-36b7-f39dabee0657
# â• â•fa0bd460-1e23-11eb-131f-f31bec373c07
# â•Ÿâ”€b405694e-1f8f-11eb-0fad-5d0c3527d206
# â• â•d497051e-1f94-11eb-2ccb-a7d3e52570d1
# â• â•cc73d620-1f8f-11eb-357d-b19b50c19280
# â• â•803ccbc0-2b53-11eb-1399-cf96cbb67727
# â•Ÿâ”€37c52e40-1e48-11eb-1e13-7ba1bebfabc4
# â•Ÿâ”€44997b82-1e48-11eb-1821-f7f3aecd8093
# â• â•41505b10-1e34-11eb-06db-750eb3ab11cf
# â• â•29b04780-1e44-11eb-3bcf-8b5f121d157f
# â• â•f81b6012-2b4a-11eb-173a-1bf763fbd71d
# â• â•d0de29e0-1e36-11eb-398c-5f0b55ea281e
# â• â•d8ab5330-1e47-11eb-06b3-af3d2ee97c7b
